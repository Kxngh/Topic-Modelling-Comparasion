{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CatBoostClassifier\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def assign_labels(input_file, output_file, useful_topics, num_clusters=4, custom_stop_words=None):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Validate presence of 'review' column\n",
    "    if 'review' not in df.columns:\n",
    "        raise ValueError(\"The input file must contain a 'review' column.\")\n",
    "\n",
    "    # 1. Preprocess the Review Text\n",
    "    def preprocess_text(text):\n",
    "        text = re.sub(r'[^\\w\\s]', '', str(text).lower())\n",
    "        return text\n",
    "\n",
    "    df['cleaned_review'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "    # Define default custom stop words if not provided\n",
    "    if custom_stop_words is None:\n",
    "        custom_stop_words = [\n",
    "            \"app\", \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
    "            \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\",\n",
    "            \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\",\n",
    "            \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\",\n",
    "            \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\",\n",
    "            \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\",\n",
    "            \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\",\n",
    "            \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\",\n",
    "            \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\",\n",
    "            \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\",\n",
    "            \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \n",
    "            \"now\", \"try\", \"keeps,\", \"still\"\n",
    "        ]\n",
    "\n",
    "    # 2. Vectorize the Review Text Using TF-IDF (with custom stop words)\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=1000, stop_words=custom_stop_words)\n",
    "    review_vectors = vectorizer.fit_transform(df['cleaned_review'])\n",
    "\n",
    "    # --- LDA Topic Modeling ---\n",
    "    lda = LatentDirichletAllocation(n_components=num_clusters, random_state=42)\n",
    "    lda.fit(review_vectors)\n",
    "\n",
    "    # Get the top words for each topic and store for review\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    topic_words = {}\n",
    "    top_words_per_topic = []  # This will store words per topic for CoherenceModel\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words_idx = topic.argsort()[:-11:-1]  # Top 10 words per topic\n",
    "        top_words = [terms[i] for i in top_words_idx]\n",
    "        topic_words[f\"Topic {topic_idx}\"] = top_words\n",
    "        # Append words as list for CoherenceModel\n",
    "        top_words_per_topic.append(top_words)\n",
    "        print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n",
    "\n",
    "    # Predict the topics for each review\n",
    "    topic_probabilities = lda.transform(review_vectors)\n",
    "    df['lda_topic'] = topic_probabilities.argmax(axis=1)\n",
    "\n",
    "    # 3. Label the Reviews as Useful or Not Useful\n",
    "    # Label reviews based on their dominant topic\n",
    "    df['is_useful'] = df['lda_topic'].apply(\n",
    "        lambda x: 1 if x in useful_topics else 0)\n",
    "\n",
    "    # Save the labeled dataset\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    # --- Evaluation Metrics ---\n",
    "    # Perplexity Score\n",
    "    perplexity = lda.perplexity(review_vectors)\n",
    "    print(f\"Model Perplexity: {perplexity}\")\n",
    "\n",
    "    # Coherence Score (using Gensim)\n",
    "    # Prepare data for coherence calculation\n",
    "    texts = df['cleaned_review'].str.split()\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    # Calculate coherence score using the top words per topic\n",
    "    coherence_model = CoherenceModel(\n",
    "        topics=top_words_per_topic, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    print(f\"Model Coherence Score: {coherence_score}\")\n",
    "\n",
    "    # Return the labeled dataframe, review vectors, topic words, and evaluation metrics\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labelling Training and testing data using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = assign_labels(input_file='csv_data.csv', output_file='training_data.csv',\n",
    "              useful_topics=[1, 3], num_clusters=4)\n",
    "\n",
    "# testing_df,testing_rv = assign_labels(input_file='unseen_data.csv', output_file='testing_data.csv', useful_topics=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "\n",
    "# 1. Load Pre-trained BERT Model and Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "# Function to get BERT embeddings for each review\n",
    "\n",
    "\n",
    "def get_bert_embeddings(reviews):\n",
    "    inputs = tokenizer(reviews, return_tensors='pt',\n",
    "                       padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Using the [CLS] token embedding (first token)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    return embeddings\n",
    "\n",
    "df = pd.read_csv('csv_data.csv')\n",
    "\n",
    "# 2. Assuming 'df' has the reviews in 'review' column\n",
    "reviews = df['review'].tolist()\n",
    "\n",
    "# 3. Get embeddings for all reviews\n",
    "embeddings = get_bert_embeddings(reviews)\n",
    "\n",
    "# 4. Reduce the dimensionality of the embeddings (e.g., using PCA)\n",
    "pca = PCA(n_components=2)  # Reduce to 2D for visualization\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# 5. Cluster the reviews using KMeans\n",
    "num_topics = 4  # Adjust this based on your needs\n",
    "kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
    "df['topic'] = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# 6. Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:,\n",
    "            1], c=df['topic'], cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(label='Topic')\n",
    "plt.title('BERT-based Topic Modeling')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.show()\n",
    "\n",
    "# 7. Get the reviews per topic\n",
    "for topic in range(num_topics):\n",
    "    topic_reviews = df[df['topic'] == topic]['review'].head(\n",
    "        5)  # Show top 5 reviews per topic\n",
    "    print(f\"Topic {topic} reviews:\\n\", topic_reviews, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
